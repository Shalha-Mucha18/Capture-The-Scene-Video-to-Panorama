# Week 1

## Neural Network Intuition
Neural networks, originally designed to mimic the brain’s learning process, have evolved into a powerful engineering tool with little resemblance to biological neurons. After cycles of popularity since the 1950s, they surged post-2005 as "deep learning," driven by big data and GPUs. Breakthroughs in speech recognition, computer vision (like ImageNet 2012), and NLP followed. While artificial neurons loosely mirror biological ones (processing inputs into outputs), modern advancements prioritize computational efficiency over brain-inspired designs. Today, neural networks thrive by leveraging massive datasets and hardware, not biological fidelity.
![image](https://github.com/user-attachments/assets/4bec0064-ba66-4893-993f-bd48f87830ef)
![image](https://github.com/user-attachments/assets/2048a32f-0886-48be-870e-53bef6feacc9)
![image](https://github.com/user-attachments/assets/531a2032-4a74-4ed0-8164-393005343420)
![image](https://github.com/user-attachments/assets/f147401e-0663-4694-a835-e0618ed20d59)
### Face Recognition Using Neural Network
![image](https://github.com/user-attachments/assets/dc3072bc-8723-44d6-9777-32be8ae29470)
### Car Classification Using Neural Network
![image](https://github.com/user-attachments/assets/b24d2aa2-8dc6-4da3-87fa-2d193ac34e9e)
### Tensorflow Implementation
![image](https://github.com/user-attachments/assets/bfde3355-dbf5-481f-801e-7045672d6256)
### Data in TensorFlow
![image](https://github.com/user-attachments/assets/a4f15aff-2598-4368-a6ab-6084c203e5ca)
## Building a neural network
In TensorFlow, building a neural network is streamlined using the Sequential API, which automatically chains layers (e.g., `Dense` with `sigmoid` activation). After defining the architecture (e.g., `model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, activation='sigmoid')])`), training involves two key steps: configuring the model with `compile()` (e.g., `model.compile(optimizer='sgd', loss='binary_crossentropy')`) and fitting it to data `(X, Y)` using `fit()` (e.g., `model.fit(X, Y, epochs=100)`). For inference, `predict()` handles forward propagation on new inputs (e.g., `predictions = model.predict(X_new)`). By convention, layers are nested directly in `Sequential()`, reducing code verbosity. While libraries abstract away manual implementation, understanding underlying mechanics, like forward propagation, remains crucial for troubleshooting and customization. This balance (leveraging frameworks for efficiency while grasping core concepts) empowers practitioners to deploy and debug neural networks confidently.  
![image](https://github.com/user-attachments/assets/fa0052d7-29b1-4b58-9a71-16ecb076dc3c)
### Digit Classification Model
![image](https://github.com/user-attachments/assets/32f025c5-9d19-4b52-8183-161911d16bbb)
### Forward propagation in a single layer

To implement forward propagation manually, we first initialize weight matrices and bias vectors. We then compute layer outputs through sequential matrix operations: multiplying inputs by weights, adding biases, and applying activation functions. These NumPy operations demonstrate how values propagate through the network. While this reveals the core mathematics behind neural networks, we typically rely on frameworks like TensorFlow for efficient, large-scale implementations. The exercise helps us understand the fundamental computations that power deep learning systems.
![image](https://github.com/user-attachments/assets/08048491-9489-4abf-b8a1-0d28b3dfaeb7)

### General implementation of forward propagation
We can generalize the forward propagation implementation by creating a reusable dense layer function. This function takes the previous layer's activations, weight matrix W (where columns represent individual neuron weights), and bias vector b as inputs. The implementation first determines the number of units in the layer from W's shape, initializes an output activation array, then iteratively computes each neuron's activation through vectorized operations. For each neuron, we extract its weights from W's columns, compute z = w·a_prev + b, and apply the sigmoid activation. By sequentially applying this dense function - first with input features X, then with each subsequent layer's outputs, we build complete forward propagation through the network. This modular approach avoids hardcoding per-neuron calculations while maintaining clarity about the underlying mathematics. Understanding this implementation helps debug neural networks even when using frameworks like TensorFlow, as it reveals the core computations these libraries optimize. The knowledge proves particularly valuable when diagnosing unexpected behavior or performance issues in practical applications.
![image](https://github.com/user-attachments/assets/d530b87d-09c8-414c-849d-c6aed6813752)
### How neural networks are implemented efficiently

The scalability of modern neural networks stems from their vectorized implementation using matrix operations. Researchers leverage parallel computing hardware like GPUs that excel at large matrix multiplications. While a naive implementation might use explicit for-loops to compute each neuron's activation separately, the vectorized approach replaces this with efficient matrix operations. By representing weights W as a matrix (where columns correspond to individual neurons' weights) and inputs X as a 2D array, forward propagation simplifies to Z = matmul(X, W) + B, followed by element-wise activation. This matmul operation performs the equivalent of all individual neuron calculations simultaneously. The vectorized implementation treats all quantities (inputs, weights, biases, and outputs) as 2D arrays/matrices, enabling hardware-accelerated parallel computation. This matrix-based approach is fundamental to training large neural networks efficiently and is why deep learning frameworks heavily optimize these linear algebra operations. Understanding this vectorization principle helps appreciate why neural networks can scale while maintaining computational efficiency.
![image](https://github.com/user-attachments/assets/d7b7ed75-c86d-4bc9-b82f-56edbcf282ad)

### Matrix multiplication
Matrix multiplication is fundamentally built upon vector dot products, systematically extended to higher dimensions. The operation between two matrices computes each element of the output as the dot product of the corresponding rows from the first matrix and columns from the second matrix. 

Key insights about matrix multiplication:
1. **Vector-Vector Case**: The dot product of vectors a and w (a·w) equals aᵀw when treating vectors as column matrices
2. **Vector-Matrix Case**: Multiplying a row vector aᵀ by matrix W computes dot products with each column of W
3. **Matrix-Matrix Case**: For AᵀW, each element (i,j) results from the dot product of row i of Aᵀ with column j of W

The transpose operation converts columns to rows, enabling proper dimensional alignment for multiplication. This systematic approach allows neural networks to compute all layer activations simultaneously through batched matrix operations, rather than processing neurons individually. The matmul operation in NumPy/TensorFlow implements precisely this pattern of grouped dot products, providing the computational efficiency essential for deep learning.
![image](https://github.com/user-attachments/assets/3ab813dc-a21a-4056-960a-785e844a1594)
![image](https://github.com/user-attachments/assets/ba076ee4-96d1-4d6a-a11c-97c90ad1177b)

### Matrix Multiplication Fundamentals

The general matrix multiplication operation follows these key principles:

1. **Dimensional Requirements**:
   - For matrix A (m×n) and W (n×p), multiplication is valid only when A's column count matches W's row count
   - Resulting matrix Z (m×p) preserves A's row dimension and W's column dimension

2. **Computation Process**:
   - Each element zᵢⱼ is computed as the dot product of row i from Aᵀ with column j from W
   - Visual grouping (shaded rows/columns) helps track which vector combinations produce each output element

3. **Neural Network Implementation**:
   - Layer activations become matrix rows (after transpose)
   - Weight matrices organize neuron parameters column-wise
   - Enables simultaneous computation of all neurons' outputs via matmul

4. **Performance Impact**:
   - Vectorized implementations leverage parallel hardware
   - Eliminates explicit neuron-by-neuron loops
   - Provides orders-of-magnitude speedup over naive implementations

5. **Implementation Insight**:
   - AᵀW computation mirrors the neural network forward pass
   - Each matmul operation computes a full layer's activations
   - Proper dimensional alignment ensures correct neuron connections

This matrix approach transforms O(n) sequential operations into O(1) parallelized computations, enabling modern deep learning scalability. The dimensional constraints naturally enforce proper network architecture connectivity.
![image](https://github.com/user-attachments/assets/9713802c-ad51-495f-83fa-2c435cd50db5)

### Matrix multiplication code
Vectorized forward prop uses `Z = matmul(Aᵀ,W) + B` to compute all neuron activations simultaneously via matrix ops, where `Aᵀ` is input, `W` contains weights (columns=neurons), and `B` is bias. The output `A_out = g(Z)` applies activation element-wise. This approach leverages parallel hardware for efficient neural network computation.
![image](https://github.com/user-attachments/assets/4bd37201-709c-49e6-9fd1-c6e43a67026d)
# Week 2
### Neural Network Training
#### TensorFlow implementation

The second week of the course covers training neural networks in TensorFlow, expanding on the previous week's inference concepts. Using handwritten digit recognition (binary classification of 0 or 1) as an example, the neural network architecture consists of an input layer, two hidden layers (25 and 15 units), and a single output unit. The training process involves three main steps: first, defining the model sequentially; second, compiling it with a specified loss function (binary cross-entropy); and third, executing the training loop using the `fit()` method with a predetermined number of epochs. Understanding the underlying mechanics, such as gradient descent and loss functions, is essential for debugging and optimizing model performance. 
![image](https://github.com/user-attachments/assets/835e6a2c-40b5-4d3e-80d5-f6a76dde5637)
### Training Details
Training a neural network in TensorFlow follows the same three-step framework used in logistic regression: first, specifying how the output is computed from input features and parameters; second, defining a loss function (like binary cross-entropy for classification or mean squared error for regression) and the cost function (average loss over the training set); and third, minimizing the cost function using gradient descent or a more advanced optimization algorithm. TensorFlow defines the model architecture by specifying layers (e.g., two hidden layers with 25 and 15 units and sigmoid activation), which determines forward propagation and initializes parameters. The loss function is set during model compilation, and the `fit` method handles optimization via backpropagation, with the number of iterations controlled by the `epochs` parameter. While TensorFlow abstracts much of the complexity, similar to how mature libraries handle sorting or matrix operations, understanding the underlying mechanics (e.g., gradient descent, backpropagation) remains crucial for debugging and improving models. Future discussions will explore enhancements like alternative activation functions (e.g., ReLU) to boost performance. The key takeaway is that while libraries simplify implementation, foundational knowledge ensures effective training and troubleshooting.
![image](https://github.com/user-attachments/assets/ddaf02c0-88f7-450e-a608-8b8043bfebb8)
![image](https://github.com/user-attachments/assets/c20e3eeb-5795-44c6-b234-8f6dec48513d)
![image](https://github.com/user-attachments/assets/5da1179a-b930-4b3d-a06f-ebb485d06b05)
### Activation Functions
![image](https://github.com/user-attachments/assets/020ee693-8e47-47bb-9789-a021aab95b37)
#### Choosing activation functions

When designing a neural network, selecting appropriate activation functions is crucial for effective learning. For the output layer, the choice depends on the problem type: sigmoid for binary classification to output probabilities, linear for regression tasks requiring positive/negative predictions, and ReLU for strictly non-negative outputs like house prices. Hidden layers overwhelmingly use ReLU due to its computational efficiency and superior training dynamics - unlike sigmoid functions that saturate in both directions and slow learning, ReLU only saturates for negative inputs while maintaining strong gradients for positive values. While alternative activations like LeakyReLU or tanh exist and may offer marginal improvements in specific cases, ReLU remains the default choice for hidden layers in most applications. This strategic selection of activation functions - matching output layer activations to problem requirements while using ReLU in hidden layers - forms a foundational aspect of neural network design that balances expressive power with training efficiency. The implementation in frameworks like TensorFlow is straightforward, with clear syntax for specifying different activations per layer, enabling practitioners to easily leverage these design principles in their models.
![image](https://github.com/user-attachments/assets/2d3c5125-1848-4dc4-87cc-109297a33f8a)
![image](https://github.com/user-attachments/assets/27669711-0f78-41ef-b4cd-5c09f720346e)

#### Why do we need activation functions?
Neural networks fundamentally rely on nonlinear activation functions to learn complex patterns - without them, even the deepest architectures would collapse into simple linear models. The key issue arises when using linear activations throughout the network: composing multiple linear transformations (Wx+b) always yields another linear function, making hidden layers mathematically redundant. For instance, a two-layer network with linear activations reduces to a single linear transformation (a2 = (w2w1)x + (w2b1+b2)), equivalent to basic linear regression. This limitation persists regardless of depth - a 10-layer "deep" network with linear activations would still behave like linear regression, completely wasting its potential. The solution lies in nonlinear activation functions like ReLU, sigmoid, or tanh, which introduce crucial nonlinearities that enable neural networks to model intricate relationships. ReLU has become the default choice for hidden layers due to its computational efficiency and ability to mitigate the vanishing gradient problem, while output layers use task-specific activations (sigmoid for binary classification, linear for regression, etc.). This strategic use of nonlinearities allows neural networks to progressively build hierarchical representations, transforming raw inputs into increasingly abstract features through each layer. Without these nonlinear activation functions, deep learning would lose its remarkable ability to approximate arbitrarily complex functions, rendering multi-layer architectures no more powerful than traditional linear models. The takeaway is clear: nonlinear activations aren't just helpful - they're essential for neural networks to fulfill their promise as universal function approximations.
![image](https://github.com/user-attachments/assets/77b5a846-760f-4e3d-b352-ca781c0eb1cf)
![image](https://github.com/user-attachments/assets/24b36d90-b995-43e0-a339-718db1617e47)

### Multiclass Classification
Multiclass classification handles problems with more than two output categories, like recognizing digits 0-9 or diagnosing multiple diseases, by extending binary classification through algorithms like softmax regression. When implemented in neural networks, it enables complex decision boundaries that partition input data into multiple distinct classes, powering applications from medical diagnosis to visual quality inspection.
![image](https://github.com/user-attachments/assets/29fdad2c-3f52-41d1-a68c-9893a030a7a8)
#### Softmax
Softmax regression generalizes logistic regression for multiclass classification by computing probabilities for each class using normalized exponentials (e^z_j/Σe^z_k), ensuring all outputs sum to 1. Its loss function (-log a_j for the true class j) incentivizes the model to maximize confidence in correct predictions, enabling neural networks to handle multiple categories effectively.
![image](https://github.com/user-attachments/assets/aa44ff1f-e3e5-43de-ba63-df1472cf9d4f)
![image](https://github.com/user-attachments/assets/8d79ea4c-01f6-4f51-8bd8-0e691cc6ccd8)
#### Neural Network with Softmax output
For multi-class classification, neural networks use a Softmax output layer that converts the final layer's Z-values into normalized probabilities across all classes. Unlike binary classification, Softmax computes each class probability using all Z-values simultaneously, ensuring probabilities sum to 1. In TensorFlow, this is implemented with SparseCategoricalCrossentropy loss and requires numerical stability tweaks for optimal performance.
![image](https://github.com/user-attachments/assets/a1f41db8-f44e-465d-b91e-5b04b702c069)
![image](https://github.com/user-attachments/assets/991a8436-3600-4613-8f19-955b4cdb1de6)
#### Improved implementation of softmax
For numerical stability, TensorFlow computes Softmax probabilities and cross-entropy loss jointly by using from_logits=True, avoiding intermediate rounding errors from large/small exponentials. The output layer uses a linear activation (raw logits, Z) while the loss function internally handles the Softmax transformation, sacrificing some readability for precision. This approach is critical for multi-class tasks but requires manually applying Softmax to logits when predicting probabilities.
![image](https://github.com/user-attachments/assets/a411b56b-acb9-4ee0-9208-7eecd4ec634f)
![image](https://github.com/user-attachments/assets/fe8fe247-48c0-4292-b241-b5a3fe266865)

#### Advanced Optimization
Adam is an adaptive optimization algorithm that dynamically adjusts individual learning rates for each parameter (e.g., w₁-w₁₀ and b) based on their gradient behavior. Unlike standard gradient descent's fixed global learning rate, Adam increases rates for parameters moving consistently in one direction while reducing rates for oscillating parameters, enabling faster and more stable convergence. To implement Adam in TensorFlow, simply specify tf.keras.optimizers.Adam(learning_rate=1e-3) as the optimizer during model compilation, making it the default choice for modern neural network training due to its robustness and efficiency.
![image](https://github.com/user-attachments/assets/0dd054a1-6dce-41d6-81b4-12c7825d01db)
![image](https://github.com/user-attachments/assets/a4d7b06d-64e8-420f-9f13-fe85fde09ac5)
#### Additional Layer Types
Convolutional layers restrict neurons to process only local regions of input data (e.g., patches of an image or segments of a time series), unlike dense layers that connect to all inputs. This architecture reduces computation, requires less training data, and mitigates overfitting by learning localized features, critical for tasks like image recognition (pixel patches) or ECG analysis (time windows). While dense layers remain versatile, convolutional neural networks (CNNs) demonstrate how specialized layer designs can optimize performance for structured data like grids (images) or sequences (signals).

# Week 3
#### Machine Learning Diagnostic
![image](https://github.com/user-attachments/assets/39e3e606-f4eb-4954-afca-28622ae78874)
#### Evaluating a model
To evaluate model performance, split the data into training (70-80%) and test sets (20-30%), then compute the training error (J_train) and test error (J_test) separately. For regression, J_test measures average squared error on unseen data, while for classification, it’s either logistic loss or misclassification rate. A large gap between low J_train and high J_test signals indicates overfitting, indicating poor generalization despite good training performance.
![image](https://github.com/user-attachments/assets/8ade67bb-f756-4d3f-a742-c5f71c15fdae)
![image](https://github.com/user-attachments/assets/3a1cc2f4-64f4-47de-a46c-cb74986480e4)
![image](https://github.com/user-attachments/assets/1e375994-d24a-4643-bae8-46911c88629f)
![image](https://github.com/user-attachments/assets/477daa8f-5234-40f4-a0fa-c018ba7bedb2)
#### Model selection and training/cross validation/test sets
To avoid biased model selection, split the data into three sets: training (60%), cross-validation (20%), and test (20%). Use the CV set to compare models (e.g., polynomial degrees or NN architectures) by selecting the one with the lowest cross-validation error (J_cv), then report final performance on the untouched test set (J_test). This ensures the test set remains an unbiased estimate of generalization error since no modeling decisions (including hyperparameters like d) are made using it.
![image](https://github.com/user-attachments/assets/fcefc87a-4ef5-491d-b6fb-91ffe1e57946)
![image](https://github.com/user-attachments/assets/e71d182d-5811-48ef-bb4d-a4ec671a4470)
#### Diagnosing bias and variance
High bias (underfitting) occurs when both training error (J_train) and validation error (J_cv) are high, indicating the model is too simple to capture patterns. High variance (overfitting) shows low J_train but significantly higher J_cv, where the model memorizes training data but fails to generalize. Plotting error curves against model complexity (e.g., polynomial degree) reveals the "Goldilocks zone" where J_cv is minimized—neither underfit nor overfit.
![image](https://github.com/user-attachments/assets/2a1fcb69-ab73-4830-9ecf-af6e63f06fd6)
![image](https://github.com/user-attachments/assets/1da68893-a844-47f6-8582-1cd26703cb3f)

#### Regularization and bias/variance
Large λ values cause high bias (underfitting) by overly penalizing weights, flattening the model (high J_train/J_cv), while λ=0 leads to high variance (overfitting) with low J_train but high J_cv. Cross-validation identifies the optimal λ (e.g., λ=0.02 here) where J_cv is minimized, balancing fit and generalization. Plotting J_train/J_cv vs λ reveals a U-shaped curve—analogous but inverted to the polynomial degree plot—with the sweet spot avoiding under/overfitting extremes.
![image](https://github.com/user-attachments/assets/e0fdbf93-5122-44fe-99c5-4ca9f08059d0)
![image](https://github.com/user-attachments/assets/78fe1c9d-3eb1-402d-b9c3-68032dc4feb2)

#### Establishing a baseline level of performance
To diagnose bias/variance, compare training error (J_train) to a baseline (e.g., human performance) and cross-validation error (J_cv) — a large gap between baseline and J_train indicates high bias, while J_cv >> J_train signals high variance. For speech recognition with human error at 10.6%, a model achieving J_train=10.8% (near-human) but J_cv=14.8% (4% higher) primarily has a variance problem, not bias. Establishing realistic baselines (especially for noisy data) reframes "high" error: even perfect models may not beat human-level benchmarks, shifting focus to generalization gaps.
![image](https://github.com/user-attachments/assets/2f4c57b7-281f-483e-93c1-1fe7d6d67f3d)
![image](https://github.com/user-attachments/assets/94f123b3-235c-41ca-b21c-133f55b0751d)
#### Learning curves
Learning curves plot training error (J_train) and validation error (J_cv) against training set size—high bias shows both errors plateauing far above baseline performance, while high variance displays a large gap between low J_train and high J_cv. For high-bias models (underfitting), increasing data won't help as curves flatten; for high-variance (overfitting), more data narrows the J_train-J_cv gap. These curves reveal whether to prioritize model complexity changes (bias) or data/regularization (variance), though generating them can be computationally expensive.
![image](https://github.com/user-attachments/assets/e3a86852-9001-49cd-8776-0d4e5a5b71d5)
![image](https://github.com/user-attachments/assets/16f92a76-116f-4bd2-aa94-b5e2ac822343)
![image](https://github.com/user-attachments/assets/6f08c5f6-3380-4447-8f2c-639a74d7efc0)
#### Deciding what to try next revisited
![image](https://github.com/user-attachments/assets/a895c95a-f9a4-47dd-953f-6e0dacce22a8)
#### Bias/variance and neural networks
Neural networks break the traditional bias-variance tradeoff by enabling low-bias models (via larger architectures) while managing variance through regularization and big data. The modern workflow iteratively scales network size until training error meets baseline performance, then addresses any remaining variance with more data/regularization, without compromising bias. This approach, powered by computational advances, makes neural networks uniquely capable of achieving high performance without strict complexity tradeoffs when properly regularized.
![image](https://github.com/user-attachments/assets/75ed23cd-6407-4328-84f4-8b8ee6adc0af)
![image](https://github.com/user-attachments/assets/ff71aace-1356-4be1-994b-310975b042ae)
#### Machine Learning Development Process
Developing an effective machine learning system, such as a spam classifier, involves an iterative cycle of architectural decisions, implementation, diagnostics, and refinement. The process begins by selecting an appropriate model architecture—whether logistic regression or a neural network—along with key hyperparameters and feature representations, such as binary word presence indicators or word counts for text classification. After initial training, diagnostics like bias/variance analysis and error examination reveal performance gaps, guiding strategic improvements: enlarging the model for high bias issues, gathering more data, or enhancing regularization for variance problems, or engineering smarter features like email header analysis or text normalization. In spam detection, this might involve deploying honeypot projects to collect labeled data, analyzing email routing patterns, or developing robust misspelling detectors. The critical insight is that systematic diagnostics, rather than intuition, should drive prioritization, preventing wasted effort on unnecessary data collection when model complexity is the true bottleneck, or vice versa. This structured approach, combining technical implementation with analytical rigor, enables efficient iteration toward optimal performance while avoiding common pitfalls in machine learning development.
![image](https://github.com/user-attachments/assets/5d7df4d9-312f-4f11-98c6-221f14f93421)
![image](https://github.com/user-attachments/assets/1c7edeba-62c8-4d9d-9fa8-d3b8c438b354)
#### Error analysis
Error analysis is a critical diagnostic tool for improving machine learning models by systematically examining misclassified examples to identify patterns and prioritize improvements. When a spam classifier misclassifies 100 out of 500 cross-validation emails, manually reviewing these errors reveals that pharmaceutical spam (21 cases) and phishing attempts (18 cases) dominate the mistakes, while deliberate misspellings (3 cases) have minimal impact—helping focus efforts on high-value fixes like collecting more pharmaceutical spam data or developing URL analysis features rather than less impactful solutions like misspelling detection.** Though labor-intensive (typically sampling 100-200 errors suffices), this human-in-the-loop process is invaluable for problems where humans can recognize patterns (like spam), as it prevents wasted effort on marginal improvements while highlighting where feature engineering or targeted data collection will yield the greatest performance gains, complementing bias/variance analysis to guide efficient model refinement.
![image](https://github.com/user-attachments/assets/975a0103-b97a-4620-939a-f0b65bb27ffe)
![image](https://github.com/user-attachments/assets/cc41adc7-d25e-4033-9ef7-05f72adca542)

#### Adding data
Effective data augmentation requires targeted strategies that address specific model weaknesses identified through error analysis. For images, techniques like rotation, scaling, and grid warping create realistic variations, while audio benefits from added background noise or connection artifacts. Synthetic data generation (e.g., creating text images with varied fonts) can dramatically expand datasets for computer vision tasks. The key is ensuring all augmented data maintains real-world relevance while efficiently improving model performance where it matters most.

#### Transfer learning: using data from a different task
Transfer learning leverages knowledge from a pre-trained model (trained on a large dataset like ImageNet) by adapting its early layers to a new task with limited data. The process involves replacing the final output layer and optionally fine-tuning some or all weights on the target dataset (e.g., handwritten digits). This works because early layers learn universal features (edges, textures) that generalize across visual tasks, while later layers capture task-specific patterns.
![image](https://github.com/user-attachments/assets/5f49d924-ba6a-4ed9-84ea-fb1a2c13c2c7)
![image](https://github.com/user-attachments/assets/c34ba3e1-2f09-42d1-bee1-8257ec1adb56)
#### Full cycle of a machine learning project
The ML project cycle begins with scoping (defining objectives), followed by iterative data collection and model training informed by error analysis, culminating in deployment with continuous monitoring. Deployment typically involves serving predictions via an inference API while logging inputs/outputs for performance tracking and model updates. MLOps practices ensure reliable scaling and maintenance, creating a feedback loop where production data can further refine the model, though ethical considerations must guide each phase.
![image](https://github.com/user-attachments/assets/6f2b946b-137c-4343-b441-6458ae331879)
![image](https://github.com/user-attachments/assets/fea54db5-634e-4dbf-8252-f718d2c1e3e2)
#### Fairness, bias, and ethics
Ethical ML requires proactive measures to prevent bias and harm, including diverse team brainstorming, pre-deployment audits for subgroup fairness, and established mitigation plans. Historical failures (biased hiring tools, discriminatory facial recognition) demonstrate the real-world consequences of unethical systems. While no simple checklist exists, continuous monitoring and industry-specific fairness standards help maintain accountability, as even well-intentioned algorithms can reinforce harmful stereotypes if not carefully designed.
![image](https://github.com/user-attachments/assets/c814d5cb-7439-4ba6-b6b1-7b376f0af72e)
































